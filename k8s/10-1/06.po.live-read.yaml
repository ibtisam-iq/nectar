# Case 1
# pod is exposed to a service, no delay.
# pod is marked as "Ready & Running", container also runs with no delay, traffic is served, no delay.
apiVersion: v1
kind: Pod
metadata:
  labels:
    app: webapp
    type: dev
  name: pod-with-no-delay-1
spec:
  containers:
  - name: abcd
    image: kodekloud/webapp-delayed-start
    ports:
    - containerPort: 8080
      protocol: TCP
---
apiVersion: v1
kind: Service
metadata:
  name: pod-with-no-delay-svc
spec:
  ports:
  - port: 8080
    protocol: TCP
    targetPort: 8080
    nodePort: 30090
  selector:
    type: dev
  type: NodePort
---
# Case 2
# pod is exposed to a service, with a delay of 80s in the container. Check the traffic serve response by this pod.

# the pod is up & exposed to the service, K8s marks it as "Ready & Running" but the container is not 
# yet running & traffic is not served till the delay is over.

# Problem: container is not running, traffic is not served, but pod is marked as "Ready & Running" by K8s.
# Solution: use a readiness probe to check if the container is ready (starts running) to serve traffic.
apiVersion: v1
kind: Pod
metadata:
  labels:
    app: webapp
    type: prod
  name: pod-with-80s-delay 
spec:
  containers:
  - name: abcd
    image: kodekloud/webapp-delayed-start
    ports:
    - containerPort: 8080
      protocol: TCP
    env:                      # env variable to delay the start of the container intentionally.
    - name: APP_START_DELAY
      value: "80"
---
apiVersion: v1
kind: Service
metadata:
  name: pod-with-80s-delay-svc
spec:
  ports:
  - port: 8080
    protocol: TCP
    targetPort: 8080
  selector:
    type: prod
  type: NodePort
---
# case 3
# pod 1 & pod 2, both exposed to a common service & see the traffic serve response by both pods simultaneouly. 
# the traffic is distributed between the two pods.
# the both pods are marked  as "Ready & Running" and traffic is sent to.
# the traffic is served by pod 1 only, but unreachable when routed to pod 2 till pod 2's container is up once the delay is over.
apiVersion: v1
kind: Pod
metadata:
  labels:
    app: webapp
    type: abc
  name: pod-with-no-delay-2
spec:
  containers:
  - name: abcd
    image: kodekloud/webapp-delayed-start
    ports:
    - containerPort: 8080
      protocol: TCP
---
apiVersion: v1
kind: Pod
metadata:
  labels:
    app: webapp
    type: abc
  name: pod-with-80s-delay-no-readiness-probe
spec:
  containers:
  - name: abcd
    image: kodekloud/webapp-delayed-start
    ports:
    - containerPort: 8080
      protocol: TCP
    env:                      
    - name: APP_START_DELAY
      value: "80"
---
apiVersion: v1
kind: Service
metadata:
  name: svc-for-both-pods
spec:
  ports:
  - port: 8080
    protocol: TCP
    targetPort: 8080
  selector:
    app: webapp
    type: abc
  type: NodePort
---
# Case 4
# as there is a delay for pod 2, so add a readiness probe into it only.

# all traffic is redirected to pod 1 (no delay) only, till the readiness probe of pod 2 becomes succesful.
# once the readiness probe of pod 2 becomes successful (container is running), traffic is 
# split & redirected to both pods. NO traffic is lost during this transition. NO DOWNTIME

# http://localhost:hostport/crash
# The one pod (let say, that pod with readiness probe) which crashes on this command, K8s will restart it 
# automatically. And, the traffic is fully routed to the other pod. Once the crashed pod is restarted, 
# traffic is split between them again.

# if a pod without a readiness probe is crashed somehow, there will be a downtime. 

# /crash ; When you hit this endpoint, the application/container crashes.
# K8s restarts it because of its restart policy.

# However, failed to passing readiness probe, k8s doesn't restart the pod, but it removes the service endpoint,
# so traffic is not routed to it.
apiVersion: v1
kind: Pod
metadata:
  labels:
    app: webapp
    type: abc
  name: pod-with-no-delay-3
spec:
  containers:
  - name: abcd
    image: kodekloud/webapp-delayed-start
    ports:
    - containerPort: 8080
      protocol: TCP
---
apiVersion: v1
kind: Pod
metadata:
  labels:
    app: webapp
    type: abc
  name: pod-with-80s-delay-and-readiness-probe
spec:
  containers:
  - name: abcd
    image: kodekloud/webapp-delayed-start
    ports:
    - containerPort: 8080
      protocol: TCP
    env:                      
    - name: APP_START_DELAY
      value: "80"
    readinessProbe:           # checks if the container is ready to handle requests.
      httpGet:                
        path: /ready          
        port: 8080 
---
apiVersion: v1
kind: Service
metadata:
  name: svc-for-both-pods-with-readiness-probe
spec:
  ports:
  - port: 8080
    protocol: TCP
    targetPort: 8080
  selector:
    app: webapp
    type: abc
  type: NodePort
---
# Case 5
# if http/localhost:hostport/freeze is executed on case 4 pods, one of the pods stops serving the traffic,
# however both pods are still marks as the same "Ready & Running", no restart.

# Problem: Both pods are still marked as "Ready & Running" even though one of them is not serving traffic.
# Solution: Use the liveness probe to check if the container is alive and running.

# Add the liveness probe to both pods.

apiVersion: v1
kind: Pod
metadata:
  labels:
    app: webapp
    type: abc
  name: pod-with-no-delay-and-liveness-probe
spec:
  containers:
  - name: abcd
    image: kodekloud/webapp-delayed-start
    ports:
    - containerPort: 8080
      protocol: TCP
    livenessProbe:           # checks if the container is alive to handle requests.
      httpGet:                
        path: /live          
        port: 8080 
---
apiVersion: v1
kind: Pod
metadata:
  labels:
    app: webapp
    type: abc
  name: pod-with-80s-delay-and-liveness-probe
spec:
  containers:
  - name: abcd
    image: kodekloud/webapp-delayed-start
    ports:
    - containerPort: 8080
      protocol: TCP
    env:                      
    - name: APP_START_DELAY
      value: "80"
    livenessProbe:              # checks if the container is alive to handle requests.
      httpGet:                
        path: /live          
        port: 8080
      initialDelaySeconds: 90   # beacuse of the 80s delay, the liveness probe will wait 90s before checking the container
      periodSeconds: 3          # checks the container every 3s
---
apiVersion: v1
kind: Service
metadata:
  name: svc-for-both-pods-with-liveness-probe
spec:
  ports:
  - port: 8080
    protocol: TCP
    targetPort: 8080
  selector:
    app: webapp
    type: abc
  type: NodePort
---
    readinessProbe:           
    # checks if the container becomes ready (starts running perfectly) to handle requests.
    # If the readiness probe fails, the container is removed from service endpoints.
      httpGet:                
        path: /ready          
        port: 8080            
      initialDelaySeconds: 5  # default is 0s
      periodSeconds: 10       # default is 10s
      failureThreshold: 3     # default is 3, after 3 consecutive failures, the container is marked as not ready.
      timeoutSeconds: 2       # default is 1s
      successThreshold: 1     # default is 1
# Determines if the container becomes ready to receive traffic. The probe starts after 5 seconds and checks the 
# readiness every 10 seconds. The probe checks if the /ready path on port 8080 is accessible. If the container
# fails the check 3 times, it is marked as "not ready" and wonâ€™t receive traffic until it succeeds.
---
    readinessProbe:
      tcpSocket:
        port: 8080                # container port
      initialDelaySeconds: 10
      periodSeconds: 20
---
    readinessProbe:
      exec:
        command:
        - cat
        - /tmp/healthy
      initialDelaySeconds: 10
      periodSeconds: 20
---
    livenessProbe:            
    # checks if the container is alive & running correctly. 
    # If liveness probe fails, Kubernetes will kill the container and restart it.
      httpGet:                # Probe uses HTTP GET to check liveness.
        path: /live           # The HTTP path to send the GET request.
        port: 8080            # The port on which the container is serving.
      initialDelaySeconds: 90 # Wait 90 seconds after the container starts before performing the first liveness check.
      periodSeconds: 10       # Perform the liveness check every 10 seconds.  # default is 10s
      failureThreshold: 3     # After 3 consecutive failures, the container is considered unhealthy, and Kubernetes will restart it.     
      timeoutSeconds: 2       # How long to wait for the probe to succeed. # if the probe takes longer than this, it will be considered a failure.
      successThreshold: 1     # Probe will be successful if it succeeds just once. # default is 1
            
# Ensures the container is alive. If this probe fails a configured number of times
# (as set by failureThreshold), Kubernetes restarts the container. In the above example, the liveness probe 
# checks if the /live path on port 8080 is accessible after an initial delay of 90 seconds. 
# It repeats this check every 10 seconds, and the pod is restarted if it fails 3 consecutive times.
---

---
# in the context of the probes (both liveness and readiness), the port refers to the container's port.
---

    

